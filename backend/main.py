#main.py
from fastapi import FastAPI, Request
from pydantic import BaseModel
from chatbot_faiss_utils import *
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
import os
import re

# LangChain ê´€ë ¨ ì¶”ê°€
from langchain_community.vectorstores import FAISS as LangChainFAISS
from langchain_openai import OpenAIEmbeddings
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_community.chat_models import ChatOpenAI
from langchain.retrievers.ensemble import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
from langchain_community.document_transformers import LongContextReorder


# OpenAI ì„ë² ë”©ë§Œ ë³„ë„ë¡œ ì“¸ ê±°ë©´ client ìœ ì§€
from openai import OpenAI

app = FastAPI()

# ì ˆëŒ€ ê²½ë¡œë¡œ static ë””ë ‰í† ë¦¬ ì§€ì •
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
STATIC_PATH = os.path.join(BASE_DIR, "app", "static")
TEMPLATE_PATH = os.path.join(BASE_DIR, "app", "templates")

app.mount("/static", StaticFiles(directory=STATIC_PATH), name="static")

templates = Jinja2Templates(directory=TEMPLATE_PATH)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ë¬¸ì„œ ë¡œë”©
documents = load_paragraphs("documents.txt")

# ì§ˆë¬¸ ì¶”ì²œ ì¸ë±ìŠ¤ ë¡œë”©
recommend_questions = load_paragraphs("question_candidates.txt")
recommend_embeddings = load_embeddings("recommend_embeddings.npy")
recommend_index = load_faiss_index("recommend_index.faiss")

class QueryRequest(BaseModel):
    query: str

# LangChainìš© FAISS + retriever ì„¤ì •
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=os.getenv("OPENAI_API_KEY"))
vectorstore = LangChainFAISS.load_local(
    "index_openai",
    embeddings=embedding_model,
    allow_dangerous_deserialization=True
)
llm = ChatOpenAI(model="gpt-4o", temperature=0.4)

# BM25 ë¦¬íŠ¸ë¦¬ë²„ìš© ë¬¸ì„œ ë³€í™˜
bm25_documents = [Document(page_content=doc) for doc in documents]
retriever_bm25 = BM25Retriever.from_documents(bm25_documents)
retriever_bm25.k = 3

# LLM ê¸°ë°˜ MultiQuery retriever
retriever_multi = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    llm=llm
)

# ì˜ë¯¸ + í‚¤ì›Œë“œ ê¸°ë°˜ ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„
retriever = EnsembleRetriever(
    retrievers=[retriever_bm25, retriever_multi],
    weights=[0.4, 0.6]
)

@app.post("/query")
async def handle_query(request: QueryRequest):
    query = request.query

    # 1. ë¬¸ì„œ ê²€ìƒ‰ (ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„)
    relevant_docs = retriever.invoke(query)

    # 2. LongContextReorderë¡œ ìˆœì„œ ì¬ì •ë ¬
    reordering = LongContextReorder()
    reordered_docs = reordering.transform_documents(relevant_docs)

    # 3. ì¶œì²˜ ì •ë³´ ì¶”ì¶œ
    retrieved_docs = []
    source_pages = []

    for doc in reordered_docs:
        text = doc.page_content
        match = re.search(r"<ì¶œì²˜:\s*(.*?)>", text)
        source = match.group(1).strip() if match else "ì¶œì²˜ ë¯¸ìƒ"
        text_clean = re.sub(r"<ì¶œì²˜:.*?>", "", text).strip()
        retrieved_docs.append(text_clean)
        source_pages.append(source)

    retrieved = "\n\n".join(retrieved_docs)
    unique_sources = sorted(set(source_pages))
    source_note = f"(ìœ„ ë‹µë³€ì€ ìˆ˜ì‹œëª¨ì§‘ìš”ê°• {', '.join(unique_sources)}ì„ ì°¸ê³ í•˜ì—¬ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.)"

    # 4. GPT í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""ë„ˆëŠ” ì„±ì‹ ì—¬ìëŒ€í•™êµì˜ ì…ì‹œ ì•ˆë‚´ë¥¼ ë„ì™€ì£¼ëŠ” ì±—ë´‡ "ìˆ˜ë£¡ì´"ì•¼.  
ì„±ì‹ ì—¬ëŒ€ë¥¼ ì§€ì›í•˜ê³ ì í•˜ëŠ” ìˆ˜í—˜ìƒê³¼ í•™ë¶€ëª¨ì—ê²Œ ì •í™•í•˜ê³  ì¹œì ˆí•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ ë„ˆì˜ ì—­í• ì´ì•¼.

[ë¬¸ì„œ ë‚´ìš©]  
{retrieved}

[ì¶œì²˜ ì •ë³´]  
{source_note}

[ì‚¬ìš©ì ì§ˆë¬¸]  
{query}

ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ ë‹µë³€ì„ ì‘ì„±í•´ì¤˜. ë°˜ë“œì‹œ **í•œêµ­ì–´**ë¡œ ë‹µí•´.

1. ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ìˆì„ ê²½ìš°, ì‹ ë¢°í•  ìˆ˜ ìˆë„ë¡ ë¬¸ì„œì— ê¸°ë°˜í•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê²Œ ì„¤ëª…í•´ì¤˜.
2. ë¬¸ì„œì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, "ìì„¸í•œ ì‚¬í•­ì€ ì„±ì‹ ì—¬ìëŒ€í•™êµ ì…í•™ì²˜ í™ˆí˜ì´ì§€ì˜ ì…ì‹œìš”ê°•ì„ ì°¸ê³ í•˜ê±°ë‚˜, ì…í•™ì²˜(02-920-2000)ì— ë¬¸ì˜í•´ ì£¼ì„¸ìš”."ë¼ëŠ” ë¬¸ì¥ì„ ê¼­ í¬í•¨ì‹œì¼œ.
3. ì§ˆë¬¸ì´ ì…ì‹œ ê´€ë ¨ì´ ì•„ë‹ˆë¼ë©´(ì˜ˆ: ì ì‹¬ ë©”ë‰´ ì¶”ì²œ, ì¡ë‹´ ë“±), ìˆ˜ë£¡ì´ë¼ëŠ” ìºë¦­í„°ë¥¼ ìœ ì§€í•˜ë©´ì„œë„ **ê°€ë³ê³  ì¹œê·¼í•˜ê²Œ ìŠ¤ëª°í† í¬**ë¡œ ë‹µí•´ì¤˜. ë‹¨, ë„ˆë¬´ ì¥í™©í•˜ê²Œ ëŠ˜ì–´ë†“ì§€ëŠ” ë§ê³  í•µì‹¬ë§Œ ì§§ê³  ìœ ì¾Œí•˜ê²Œ ë§í•´.
4. ëª¨ë“  ëŒ€ë‹µì€ ìˆ˜ë£¡ì˜ ì •ì²´ì„±(ì„±ì‹ ì—¬ëŒ€ ë„ìš°ë¯¸, ì¹œì ˆí•˜ê³  ë˜‘ë˜‘í•œ ìš© ìºë¦­í„°)ì„ ìœ ì§€í•œ ë§íˆ¬ë¡œ ì‘ì„±í•´ì¤˜.
5. ì…ì‹œ ì •ë³´ì— ëŒ€í•´ ë‹µë³€í•  ë•Œ ì¸ì‚¿ë§ì€ ë§¤ë²ˆ í•˜ì§€ ì•Šì•„ë„ ë¼.
6. ë¬¸ì„œì—ì„œ ê°€ì ¸ì˜¨ ì •ë³´ë¡œ ë‹µë³€í•  ë•ŒëŠ” ê·¼ê±°ë¡œ ì‚¬ìš©ëœ ë¬¸ë‹¨ì˜ ì¶œì²˜ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´, ì¶œì²˜ë¥¼ ë”°ë¡œ ë¬¸ì¥ë§ˆë‹¤ ë„£ì§€ ë§ê³ , ë‹µë³€ ë§ˆì§€ë§‰ ì¤„ì— ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í•œ ë²ˆë§Œ ìš”ì•½í•´ì„œ ë„£ì–´ì¤˜:
(ìœ„ ë‹µë³€ì€ ìˆ˜ì‹œëª¨ì§‘ìš”ê°• p.16,p.17ì„ ì°¸ê³ í•˜ì—¬ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.)
7. ë§Œì•½ ì§ˆë¬¸ì´ ë‹¨ì–´ í•˜ë‚˜ë§Œ í¬í•¨ëœ ë„ˆë¬´ ì§§ì€ ì§ˆë¬¸ì´ê±°ë‚˜, ì˜ˆë¥¼ ë“¤ì–´ "ëª¨ì§‘ì¸ì›"ì²˜ëŸ¼ ë¶ˆë¶„ëª…í•œ í‚¤ì›Œë“œë§Œ ìˆì„ ê²½ìš°ì—ëŠ” ì•„ë˜ì²˜ëŸ¼ ë‹µë³€í•´ì¤˜:
ì£„ì†¡í•´ìš”, ì§ˆë¬¸ì´ ì¡°ê¸ˆ ë¶ˆë¶„ëª…í•´ìš”. ì–´ë–¤ ëª¨ì§‘ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ë” ì •í™•í•˜ê²Œ ì•ˆë‚´í•´ë“œë¦´ ìˆ˜ ìˆì–´ìš”! ğŸ˜Š
"""

    chat_response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.4,
        top_p=0.95,
        presence_penalty=0.6,
        frequency_penalty=0.3
    )

    return {"answer": chat_response.choices[0].message.content}

@app.post("/suggest")
async def recommend_questions_endpoint(request: QueryRequest):
    query = request.query

    embedding_response = client.embeddings.create(
        input=query,
        model="text-embedding-3-small"
    )
    query_embedding = np.array(embedding_response.data[0].embedding)
    query_embedding = query_embedding / np.linalg.norm(query_embedding)

    top_k = 3
    scores, indices = recommend_index.search(np.array([query_embedding]), top_k)
    similar_questions = [recommend_questions[idx] for idx in indices[0]]

    return {"results": similar_questions}

@app.get("/", response_class=HTMLResponse)
async def serve_index(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/chat", response_class=HTMLResponse)
async def serve_chat(request: Request):
    return templates.TemplateResponse("chat.html", {"request": request})
