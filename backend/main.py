#main.py
from fastapi import FastAPI, Request
from pydantic import BaseModel
from chatbot_faiss_utils import *
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
import os
import re
import numpy as np
import statistics
from openai import OpenAI
# LangChain
from langchain_community.vectorstores import FAISS as LangChainFAISS
from langchain_openai import OpenAIEmbeddings
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_community.chat_models import ChatOpenAI

app = FastAPI()

# ì ˆëŒ€ ê²½ë¡œë¡œ static ë””ë ‰í† ë¦¬ ì§€ì •
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
STATIC_PATH = os.path.join(BASE_DIR, "app", "static")
TEMPLATE_PATH = os.path.join(BASE_DIR, "app", "templates")

app.mount("/static", StaticFiles(directory=STATIC_PATH), name="static")

templates = Jinja2Templates(directory=TEMPLATE_PATH)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ì…ì‹œ ì •ë³´ ë¬¸ì„œ ë¡œë”©
documents = load_paragraphs("documents.txt")

# ì§ˆë¬¸ ì¶”ì²œ ë¬¸ì„œ, ì¸ë±ìŠ¤ ë¡œë”©
recommend_questions = load_paragraphs("question_candidates.txt")
recommend_embeddings = load_embeddings("recommend_embeddings.npy")
recommend_index = load_faiss_index("recommend_index.faiss")

class QueryRequest(BaseModel):
    query: str

# LangChainìš© FAISS + retriever ì„¤ì •
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=os.getenv("OPENAI_API_KEY"))
vectorstore = LangChainFAISS.load_local(
    "index_openai",
    embeddings=embedding_model,
    allow_dangerous_deserialization=True
)
llm = ChatOpenAI(model="gpt-4o", temperature=0.4)

# ì–´ë–¤ ë¬¸ë‹¨(hash ê°’)ì´ FAISS ì¸ë±ìŠ¤ì—ì„œ ëª‡ ë²ˆì§¸(row)ì— ì €ì¥ë¼ ìˆëŠ”ì§€ ë§¤í•‘
DOCROW_BY_HASH = {}
# ì¸ë±ìŠ¤ì˜ í–‰ ë²ˆí˜¸(i)ì™€ í•´ë‹¹ í–‰ì˜ ë¬¸ë‹¨ ID(doc_id)ë¥¼ ìˆœíšŒ
for i, doc_id in enumerate(vectorstore.index_to_docstore_id):
    doc = vectorstore.docstore.search(doc_id)
    if doc:
        # docì´ ë¬¸ìì—´ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ í•´ì‹œê°’ ê³„ì‚°
        DOCROW_BY_HASH[hash(doc)] = i

# íŠ¹ì • ë¬¸ë‹¨(doc)ì˜ ì„ë² ë”© ë²¡í„°ë¥¼ FAISSì—ì„œ ì§ì ‘ êº¼ë‚´ê¸°
def get_doc_vector_from_faiss(doc):
    # docë„ ë¬¸ìì—´ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ í•´ì‹œê°’ ê³„ì‚°
    row = DOCROW_BY_HASH.get(hash(doc))
    if row is None:
        return None
    try:
        vec = vectorstore.index.reconstruct(row)
    except Exception:
        return None
    vec = np.asarray(vec, dtype="float32")  # ë„˜íŒŒì´ ë°°ì—´ë¡œ ë³€í™˜
    return vec / (np.linalg.norm(vec) + 1e-12)  # L2 ì •ê·œí™”í•´ì„œ ë°˜í™˜

# MultiQuery retriever(ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ì§ˆë¬¸ ìƒì„±)
retriever = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    llm=llm)

# -----------------------------------------------------------------------------
# ì´ ì„¤ì •ë“¤ì€ ë‹µë³€ì„ ìƒì„±í•œ ë’¤, í›„ë³´ ë¬¸ë‹¨ë“¤ê³¼ ì„ë² ë”© ìœ ì‚¬ë„ë¥¼ ë¹„êµí•˜ì—¬ ì¶œì²˜ë¥¼ ë¶™ì¼ì§€ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

# POST_HOC_TOP_K        : ìœ ì‚¬ë„ ìƒìœ„ ëª‡ ê°œ ë¬¸ë‹¨ì„ í›„ë³´ë¡œ ë³¼ì§€ ê²°ì •
# POST_HOC_MIN_SCORE    : í›„ë³´ ì¤‘ ìµœëŒ€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ì´ ê°’ë³´ë‹¤ ë‚®ìœ¼ë©´ ì¶œì²˜ë¥¼ ë¶™ì´ì§€ ì•ŠìŒ
# POST_HOC_MIN_STD      : ìƒìœ„ kê°œ ì ìˆ˜ì˜ í‘œì¤€í¸ì°¨ê°€ ì´ ê°’ë³´ë‹¤ ì‘ìœ¼ë©´ ëœë¤ ë§¤ì¹­ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì¶œì²˜ë¥¼ ë¶™ì´ì§€ ì•ŠìŒ
# POST_HOC_MIN_ANSWER_LEN: ë‹µë³€ ê¸¸ì´ê°€ ì´ ë¬¸ì ìˆ˜ë³´ë‹¤ ì§§ìœ¼ë©´ ì¶œì²˜ë¥¼ ë¶™ì´ì§€ ì•ŠìŒ
# QDOC_MIN/ADOC_MIN     : ì§ˆë¬¸â†”ë¬¸ë‹¨ / ë‹µë³€â†”ë¬¸ë‹¨ ìµœì†Œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ì¤€
# TOP1_MARGIN           : ìµœê³ ì ê³¼ ë‘ ë²ˆì§¸ ì ìˆ˜ ì°¨ì´ê°€ ì´ ê°’ ì´ìƒì´ì–´ì•¼ í•¨
# EMB_CACHE             : ë¬¸ì„œ ì„ë² ë”© ìºì‹œ. ë¬¸ë‹¨ì„ ì„ë² ë”©í•  ë•Œë§ˆë‹¤ API í˜¸ì¶œì„ ì¤„ì´ê¸° ìœ„í•´ ì‚¬ìš©
# parse_inline_source   : ë¬¸ë‹¨ ë³¸ë¬¸ì—ì„œ <ì¶œì²˜: ...> í˜•íƒœì˜ ì¶œì²˜ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
# should_attach_citation: ì¶œì²˜ë¥¼ ë¶™ì¼ì§€ ë§ì§€, ì¡°ê±´ì„ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
# -----------------------------------------------------------------------------

POST_HOC_TOP_K = 4
POST_HOC_MIN_SCORE = 0.28
POST_HOC_MIN_STD = 0.035
POST_HOC_MIN_ANSWER_LEN = 25

QDOC_MIN = 0.28
ADOC_MIN = 0.26
TOP1_MARGIN = 0.03

# ì„ë² ë”© ìºì‹œ (ì „ì—­)
EMB_CACHE = {}  # key: ë¬¸ì„œ ê³ ìœ ID(ë˜ëŠ” ë‚´ìš© í•´ì‹œ), value: L2 ì •ê·œí™”ëœ np.array ë²¡í„°

def parse_inline_source(text: str) -> str:
    m = re.search(r"<\s*ì¶œì²˜[:ï¼š]\s*([^>]+)>", text)
    return m.group(1).strip() if m else ""

def should_attach_citation(scores, answer_text) -> bool:
    if not scores:
        return False
    # ê¸¸ì´ê°€ POST_HOC_MIN_ANSWER_LENë³´ë‹¤ ì§§ìœ¼ë©´ ì¶œì²˜ë¥¼ ë¶™ì´ì§€ ì•ŠìŒ
    if len(answer_text.strip()) < POST_HOC_MIN_ANSWER_LEN:
        return False
    # ìœ ì‚¬ë„ê°€ POST_HOC_MIN_SCORE ë³´ë‹¤ ë‚®ìœ¼ë©´ ì¶œì²˜ë¥¼ ë¶™ì´ì§€ ì•ŠìŒ
    top_score = max(scores)
    if top_score < POST_HOC_MIN_SCORE:
        return False
    # í‘œì¤€í¸ì°¨ê°€ POST_HOC_MIN_STDë³´ë‹¤ ì‘ìœ¼ë©´ ì¶œì²˜ë¥¼ ë¶™ì´ì§€ ì•ŠìŒ
    try:
        stdv = statistics.pstdev(scores)
    except statistics.StatisticsError:
        stdv = 0.0
    if stdv < POST_HOC_MIN_STD:
        return False
    #ë‹¤ í†µê³¼í•˜ë©´ ì¶œì²˜ë¥¼ ë¶™ì„
    return True

#ë‹µë³€ ìƒì„± ìš”ì²­
@app.post("/query")
async def handle_query(request: QueryRequest):
    query = request.query

    # ë¬¸ì„œ ê²€ìƒ‰ (retriever)
    relevant_docs = retriever.invoke(query)

    # relevant_docsì˜ ê°ê°ì— í¬í•¨ëœ <ì¶œì²˜: ...> ì œê±°
    retrieved_docs = []
    for doc in relevant_docs:
        text = doc.page_content
        text_clean = re.sub(r"<\s*ì¶œì²˜[:ï¼š][^>]+>", "", text).strip()
        retrieved_docs.append(text_clean)
    retrieved = "\n\n".join(retrieved_docs) #ë¬¸ë‹¨ë“¤ì„ í•©ì³ì„œ gptì—ê²Œ ë³´ëƒ„

    # í”„ë¡¬í”„íŠ¸
    prompt = f"""
            [ì‚¬ìš©ì ì§ˆë¬¸]  
            {query}

            [ê´€ë ¨ ë‚´ìš©]  
            {retrieved}

            ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì¤˜.

            1. ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, "ìì„¸í•œ ì‚¬í•­ì€ ì„±ì‹ ì—¬ìëŒ€í•™êµ ì…í•™ì²˜ í™ˆí˜ì´ì§€ì˜ ì…ì‹œìš”ê°•ì„ ì°¸ê³ í•˜ê±°ë‚˜, ì…í•™ì²˜(02-920-2000)ì— ë¬¸ì˜í•´ ì£¼ì„¸ìš”."ë¼ëŠ” ë¬¸ì¥ì„ í¬í•¨ì‹œì¼œ.
            2. ì…ì‹œ ê´€ë ¨ ì§ˆë¬¸ì´ ì•„ë‹ˆë¼ë©´(ì˜ˆ: ì ì‹¬ ë©”ë‰´ ì¶”ì²œ, ì¡ë‹´ ë“±), **ê°€ë³ê³  ì¹œê·¼í•˜ê²Œ ìŠ¤ëª°í† í¬**ë¡œ ë‹µí•´ì¤˜.
            3. ì¸ì‚¿ë§ì€ ë§¤ë²ˆ í•˜ì§€ ì•Šì•„ë„ ë¼.
            4. ì˜ë„ë¥¼ ì•Œ ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ì´ë‚˜ í‚¤ì›Œë“œë§Œ ìˆì„ ê²½ìš°ì—ëŠ” ì´ë ‡ê²Œ ë‹µë³€í•´ì¤˜: "ì£„ì†¡í•´ìš”, ì§ˆë¬¸ì´ ì¡°ê¸ˆ ë¶ˆë¶„ëª…í•´ìš”. êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ë” ì •í™•í•˜ê²Œ ì•ˆë‚´í•´ë“œë¦´ ìˆ˜ ìˆì–´ìš”! ğŸ˜Š"
            5. í•™ê³¼ëª…ì´ë‚˜ ì „í˜•ëª…ì„ ì–¸ê¸‰í•˜ì§€ ì•Šê³  ëª¨ì§‘ ì¸ì›ì— ëŒ€í•œ ì§ˆë¬¸ì„ í•˜ë©´, ëª…ì‹œí•´ì„œ ë‹¤ì‹œ ë¬¼ì–´ë´ë‹¬ë¼ê³  í•´ì¤˜.
            6. í•™ìƒë¶€ êµê³¼ ì „í˜•ì˜ ëª¨ì§‘ì¸ì›ì— ê´€í•œ ì§ˆë¬¸ì´ë¼ë©´ ë‹¤ìŒì„ ì°¸ê³  í•´ì„œ ë‹µí•´ì¤˜:
                í•™ìƒë¶€ êµê³¼ ì „í˜•ìœ¼ë¡œëŠ” ê°„í˜¸ëŒ€í•™(ìì—°), ì‚¬ë²”ëŒ€í•™ ì™¸ì˜ í•™ê³¼ë¥¼ ì œì™¸í•˜ê³  ëª¨ì§‘í•˜ì§€ ì•Šì•„.
            7. ëª¨ì§‘ë‹¨ìœ„ë¥¼ ì–¸ê¸‰í•˜ì§€ ì•Šê³  íŠ¹ì„±í™”ê³ êµì¶œì‹ ì ê¸°ì¤€í•™ê³¼ì— ëŒ€í•œ ì§ˆë¬¸ì„ í•˜ë©´ , ëª…ì‹œí•´ì„œ ë‹¤ì‹œ ë¬¼ì–´ë³´ë¼ê³  ì•ˆë‚´í•´ì¤˜.
                """

    chat_response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "system", "content": "ë„ˆëŠ” ì„±ì‹ ì—¬ìëŒ€í•™êµì˜ ì…ì‹œ ì•ˆë‚´ë¥¼ ë„ì™€ì£¼ëŠ” ì±—ë´‡ 'ìˆ˜ë£¡ì´'ì•¼. ìˆ˜í—˜ìƒì—ê²Œ ì¹œì ˆí•˜ê²Œ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ ë„ˆì˜ ì—­í• ì´ì•¼. ëª¨ë“  ëŒ€ë‹µì€ ìˆ˜ë£¡ì˜ ì •ì²´ì„±(ì„±ì‹ ì—¬ëŒ€ ë„ìš°ë¯¸, ì¹œì ˆí•˜ê³  ë˜‘ë˜‘í•œ ìš© ìºë¦­í„°)ì„ ìœ ì§€í•œ ë§íˆ¬ë¡œ ì‘ì„±í•´ì¤˜."},
                  {"role": "user", "content": prompt}],
        temperature=0.4,
        top_p=0.95,
        presence_penalty=0.6,
        frequency_penalty=0.3
    )

    # gpt ë‹µë³€
    answer = chat_response.choices[0].message.content
    # gptê°€ ì„ì˜ë¡œ ì‚½ì…í–ˆì„ ìˆ˜ ìˆëŠ” <ì¶œì²˜: ...>  ì œê±°
    answer = re.sub(r"<\s*ì¶œì²˜[:ï¼š][^>]+>", "", answer).strip()



    # 'í˜„ì¬ ê²€ìƒ‰ëœ ë¬¸ì„œ'ì™€ 'gpt ë‹µë³€','ì§ˆë¬¸' ì„ë² ë”©ì„ ë¹„êµí•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ì¶©ë¶„íˆ ë†’ì€ ë¬¸ë‹¨ë§Œ ì¶œì²˜ë¥¼ ë¶™ì¸ë‹¤.
    try:
        cand_docs = relevant_docs  # í˜„ì¬ ê²€ìƒ‰ëœ ë¬¸ë‹¨ë§Œ ë¹„êµ ëŒ€ìƒìœ¼ë¡œ ì‚¬ìš©
        if not cand_docs:
            raise RuntimeError("No candidate docs for post-hoc matching")


        # ë‹µë³€ <-> ë¬¸ë‹¨
        # ë‹µë³€ ì„ë² ë”© + L2 ì •ê·œí™”(ë²¡í„°ì˜ ê¸¸ì´ë¥¼ 1ë¡œ)
        answer_vec = embedding_model.embed_query(answer)
        a_vec = np.array(answer_vec, dtype="float32")
        a_vec /= (np.linalg.norm(a_vec) + 1e-12)
        # ë¬¸ë‹¨ ë²¡í„° ê°€ì ¸ì˜¤ê¸° + L2 ì •ê·œí™”
        def _doc_key(d):
            return hash(d.page_content)
        cand_embs = []
        for d in cand_docs:
            v = get_doc_vector_from_faiss(d) #FAISS ì¸ë±ìŠ¤ì— ì €ì¥ë¼ ìˆëŠ” ë²¡í„° êº¼ë‚´ê¸°
            if v is None: #ëª»ê°€ì ¸ì˜¨ ê²½ìš°, EMB_CACHEì— ìˆëŠ”ì§€ í™•ì¸
                k = _doc_key(d)
                v = EMB_CACHE.get(k)
                if v is None: #ì—†ìœ¼ë©´ ìƒˆë¡œ ì„ë² ë”©, ìºì‹œì— ì €ì¥
                    v_list = embedding_model.embed_documents([d.page_content])[0]
                    v = np.array(v_list, dtype="float32")
                    v /= (np.linalg.norm(v) + 1e-12)
                    EMB_CACHE[k] = v
            cand_embs.append(v) #ë²¡í„°ë¥¼ cand_embsì— ì¶”ê°€
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„: ë‹µë³€ <-> ë¬¸ë‹¨
        scores_ans_doc = [float(np.dot(a_vec, v)) for v in cand_embs]


        # ì§ˆë¬¸ <-> ë¬¸ë‹¨
        # ì§ˆë¬¸ ì„ë² ë”© + ì •ê·œí™”
        query_vec = embedding_model.embed_query(query)
        q_vec = np.array(query_vec, dtype="float32")
        q_vec /= (np.linalg.norm(q_vec) + 1e-12)
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„: ì§ˆë¬¸ <-> ë¬¸ë‹¨
        scores_q_doc = [float(np.dot(q_vec, v)) for v in cand_embs]


        # ìƒìœ„ TOP_K ë¬¸ë‹¨ ì„ íƒ
        ranked = sorted(
            zip(cand_docs, scores_ans_doc, scores_q_doc),
            key=lambda x: x[1],
            reverse=True
        )
        posthoc = ranked[:POST_HOC_TOP_K] #ìƒìœ„ Kë§Œí¼ ì˜ë¼ì„œ posthocì— ì €ì¥
        topk_ans = [s_ad for _, s_ad, _ in posthoc] #ë‘ ë²ˆì§¸ ê°’(s_ad)(ë‹µë³€ <-> ë¬¸ë‹¨ ìœ ì‚¬ë„ ì ìˆ˜)ë§Œ ì¶”ì¶œ
        topk_q   = [s_qd for _, _, s_qd in posthoc] #ì„¸ ë²ˆì§¸ ê°’(s_qd)(ì§ˆë¬¸ <-> ë¬¸ë‹¨ ìœ ì‚¬ë„ ì ìˆ˜)ë§Œ ì¶”ì¶œ

        # ë‹µë³€ <-> ë¬¸ë‹¨ ìœ ì‚¬ë„ ì ìˆ˜ë“¤ì˜ í‘œì¤€í¸ì°¨ ê³„ì‚° (í´ìˆ˜ë¡ í›„ë³´ í™•ì‹¤)
        try:
            stdv = statistics.pstdev(topk_ans) if len(topk_ans) > 1 else 0.0
        except statistics.StatisticsError:
            stdv = 0.0
        # 1, 2ë“± ì ìˆ˜ë¥¼ ë½‘ìŒ
        top1 = topk_ans[0] if topk_ans else 0.0
        top2 = topk_ans[1] if len(topk_ans) > 1 else 0.0
        # ë‘˜ì˜ ì°¨ì´ê°€ TOP1_MARGINë³´ë‹¤ í° ì§€ (í´ìˆ˜ë¡ í›„ë³´ í™•ì‹¤)
        margin_ok = (top1 - top2) >= TOP1_MARGIN

        # ì§ˆë¬¸ <-> ë¬¸ë‹¨, ë‹µë³€ <-> ë¬¸ë‹¨ì˜ ìµœê³  ìœ ì‚¬ë„ê°€ ìµœì†Œ ê¸°ì¤€ ì´ìƒì¸ì§€ í™•ì¸
        qdoc_ok = (max(topk_q)   if topk_q   else 0.0) >= QDOC_MIN
        adoc_ok = (max(topk_ans) if topk_ans else 0.0) >= ADOC_MIN

        # ê¸°ì¡´ ê¸°ì¤€ (ë‹µë³€ ê¸¸ì´, ìµœê³ ì , ë¶„ì‚°) ë§Œì¡±í•˜ëŠ”ì§€
        attach_basic = should_attach_citation(topk_ans, answer)

        # ìµœì¢… ë¶€ì°© ì—¬ë¶€ ê²°ì •
        attach = (
                len(answer.strip()) >= POST_HOC_MIN_ANSWER_LEN and
                qdoc_ok and adoc_ok and
                (attach_basic or margin_ok)
        )

        if attach:
            # ìƒìœ„ K ì¤‘ ì²« ë²ˆì§¸ ë¬¸ë‹¨ì—ì„œ ì¶œì²˜ë§Œ ì¶”ì¶œ
            top_doc, _, _ = posthoc[0]
            inline = parse_inline_source(top_doc.page_content)
            if inline:
                citation_sentence = f"(ë³¸ ë‚´ìš©ì€ 2026 ìˆ˜ì‹œëª¨ì§‘ìš”ê°• {inline}ë¥¼ ì°¸ê³ í•˜ì—¬ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.)"
                answer = answer.rstrip() + " " + citation_sentence


    except Exception:
        # ì‚¬í›„ ë§¤ì¹­ ë„ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì¶œì²˜ ë¶€ì°©ì„ ê±´ë„ˆëœë‹ˆë‹¤.
        answer += ""

    # ìµœì¢… ë‹µë³€ ë°˜í™˜
    return {"answer": answer}


#ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ì¶”ì²œ ì§ˆë¬¸ ì„ ì •, í”„ë¡ íŠ¸ì— ë³´ë‚´ê¸°
@app.post("/suggest")
async def recommend_questions_endpoint(request: QueryRequest):
    query = request.query

    embedding_response = client.embeddings.create(
        input=query,
        model="text-embedding-3-small"
    )
    #ì‚¬ìš©ì ì§ˆë¬¸ ì„ë² ë”©
    query_embedding = np.array(embedding_response.data[0].embedding)
    query_embedding = query_embedding / np.linalg.norm(query_embedding)

    # ì¶”ì²œ ì§ˆë¬¸ í›„ë³´ë¥¼ 10ê°œë¡œ ë½‘ìŒ
    top_k = 10
    scores, indices = recommend_index.search(np.array([query_embedding], dtype=np.float32), top_k)
    # ì„ê³„ê°’
    THRESH = 0.35
    # (ì ìˆ˜, ì¸ë±ìŠ¤) ìŒì„ ë§Œë“¤ì–´ ì„ê³„ì¹˜ ì´ìƒë§Œ ë‚¨ê¹€
    pairs = [(float(scores[0][i]), int(indices[0][i])) for i in range(len(indices[0]))]
    filtered = [(s, idx) for (s, idx) in pairs if s >= THRESH]
    # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ 3ê°œë§Œ ë°˜í™˜
    filtered.sort(key=lambda x: x[0], reverse=True)
    filtered = filtered[:3]
    similar_questions = [recommend_questions[idx] for (s, idx) in filtered]
    return {"results": similar_questions}

#ë©”ì¸ í™”ë©´
@app.get("/", response_class=HTMLResponse)
async def serve_index(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

#ì±„íŒ… í™”ë©´
@app.get("/chat", response_class=HTMLResponse)
async def serve_chat(request: Request):
    return templates.TemplateResponse("chat.html", {"request": request})

#FAQ
@app.get("/jungsi_faq", response_class=HTMLResponse)
async def serve_jungsi_faq(request: Request):
    return templates.TemplateResponse("jungsi_faq.html", {"request": request})
